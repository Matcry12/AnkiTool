# Anki Connection Settings
# For Docker on Windows/Mac, use host.docker.internal
# For Docker on Linux with host network, use localhost or actual IP
# For remote Anki, use the actual IP address
ANKI_HOST=host.docker.internal
ANKI_PORT=8765

# LLM Provider Settings
# Options: gemini, openai, custom
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.0-flash-exp

# API Keys (required based on provider)
# Get Gemini API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Get OpenAI API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Custom LLM Settings (for local models like Ollama)
# Example for Ollama running in Docker:
# CUSTOM_ENDPOINT=http://ollama:11434/v1
# Example for Ollama on host:
# CUSTOM_ENDPOINT=http://host.docker.internal:11434/v1
CUSTOM_ENDPOINT=http://localhost:11434/v1
CUSTOM_MODEL=llama2
CUSTOM_API_KEY=dummy-key